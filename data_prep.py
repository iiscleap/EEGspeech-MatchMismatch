# -*- coding: utf-8 -*-
# @Author: aksharasoman
# @Date:   2022-03-30 15:54:46
# @Last Modified by:   aksharasoman
# @Last Modified time: 2022-10-29 15:07:29
import math, random
import numpy as np
import scipy.io
import soundfile as sf
from gensim.models import KeyedVectors

import utils 
import torch
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence     
# from keras.utils import pad_sequences
from config import *

#---------- Feature Generation ----------#

# segment signal based on time boundaries
def segment_signal(signal,onset_time,offset_time,fs):
    # Returns list of segments 
    segments = []
    for k in range(len(onset_time)):
        start_sample = int(round(onset_time[k]*fs))
        end_sample = int(round(offset_time[k]*fs))
        segments.append(signal[start_sample:end_sample])
    return segments

# segment eeg signal based on time boundaries
def segment_eeg_signal(signal,onset_time,offset_time,fs):
    # Returns list of segments 
    segments = []
    for k in range(len(onset_time)):
        start_sample = int(round(onset_time[k]*fs))
        end_sample = int(round(offset_time[k]*fs))
        segments.append(signal[:,start_sample:end_sample]) # 0th dim: channels
    return segments

#---------- Speech related
def mel2hz(mel):
    # f=700(10^{m/2595}âˆ’1)
    t = pow(10, mel / 2595) - 1
    return 700 * t

def hz2mel(hz):
    # m=2595log10(1+f/700)
    t = np.log10(hz / 700 + 1)
    return 2595 * t

# python function that returns sub-band envelopes generated by mel scaled filter bank processing 
    #NB: Use Mel-scaled filter banks if the machine learning algorithm is not susceptible to highly correlated input.
def melFB(signal,fs,nfilt):
    
    """
    Inputs:
    signal: audio signal (already read)
    fs: sampling freq of audio signal
    nfilt: number of mel filters
    """
    # Hyper-parameters
    pre_emphasis = 0.97
    NFFT = 512
    low_freq, high_freq = 0, 8000 # mel filterbank in 0-8kHz range
    win_length = 0.03125  # 31.25ms
    win_stride = 0.015625  # 15.625ms stride : 15.625ms overlap
    """ NB: frame length & stride are decided to resample to obtain 
            64 samples per sec (64Hz) - irrespective of input signal fs.
            64Hz is chosen to closely follow cheveigne et.al & jaswanth et.al
            preprocessing steps."""

    # 2. Pre-emphasis
    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])

    # 3. Framing
    win_length, win_step = win_length * fs, win_stride * fs # Convert from seconds to samples
    signal_length = len(emphasized_signal)
    win_length = int(round(win_length))
    win_step = int(round(win_step))
    num_frames = int(
        np.ceil(float(np.abs(signal_length - win_length)) / win_step)
    )  # make sure we have atleast 1 frame

    pad_signal_length = num_frames * win_step + win_length
    z = np.zeros((pad_signal_length - signal_length))
    pad_signal = np.append(
        emphasized_signal, z
    )  # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal

    indices = (
        np.tile(np.arange(0, win_length), (num_frames, 1))
        + np.tile(
            np.arange(0, num_frames * win_step, win_step), (win_length, 1)
        ).T
    )

    frames = pad_signal[indices.astype(np.int32, copy=False)]

    # 4. Fourier Transform & Power Spectrum
    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # magnitude of the FFT
    pow_frames = (1 / NFFT) * (mag_frames ** 2)  # power spectrum

    # 5. Filter banks
        # Applying triangular filters on a mel scale to the power spectrum to extract frequency bands
    if nfilt > 0:
        low_freq_mel = hz2mel(low_freq)
        high_freq_mel = hz2mel(high_freq)
        mel_points = np.linspace(
            low_freq_mel, high_freq_mel, nfilt + 2
        )  # Equally spaced in mel scale
        hz_points = mel2hz(mel_points)
        bin = np.floor((NFFT + 1) * hz_points / fs)

        fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))
        for m in range(1, nfilt + 1):
            f_m_minus = int(bin[m - 1])  # left
            f_m = int(bin[m])  # center
            f_m_plus = int(bin[m + 1])  # right

            for k in range(f_m_minus, f_m):
                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])
            for k in range(f_m, f_m_plus):
                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])

        filter_banks = np.dot(pow_frames, fbank.T) 
    else: 
        filter_banks = pow_frames # spectrogram alone
    
    filter_banks = np.where(
        filter_banks == 0, np.finfo(float).eps, filter_banks
    )  # Numerical Stability
    filter_banks = 20 * np.log10(filter_banks)  # dB

     # 6. Mean Normalization
    filter_banks -= np.mean(filter_banks, axis=0) + 1e-8

    return filter_banks

def get_frames(feat,frame_length, frame_step, fs):
    # feat: feature which is to be divided into different decision windows/frames
    # input feat shape: time_samples x featDimension
    # frame_length: decision window duration in seconds
    # frame_step: step length in seconds
    # fs: sampling frequency of feature 

    # frames: returns list of frames; each frame of size: featDim x num_frame (transpose)
    num_frame = math.floor(frame_length * fs) # number of samples in a frame
    num_step = math.floor(frame_step * fs)
    frames = []
    
    num_start = 0
    num_end = num_frame 
    while num_end <= feat.shape[0]:
        frames.append(np.transpose(feat[num_start:num_end]))
        num_start += num_step
        num_end += num_step
    return frames 

def generate_melSpec_frames(dataPath,fileId,n_melFilt=28,frame_length=5,frame_step = 0.5,fs=64):
    
    # input:
    # n_melFilt: number of mel filters for mel spectrogram computation
    # frame_length : duration of decision frame (used for training)
    
    # output: 
    #   frames: list of feature frames
    #       each frame shape: time_samples  x num_MelFilters
    
    fname = f'{dataPath}/16khz_audio{fileId}.wav'
    
    melspec = melFB(fname,n_melFilt) # (use default frame_length,frame_stride to obtain 64Hz fs)
    #> Shape: (num_samples,28)

    # Mean and variance normalization
    melspec_norm, mel_mean ,mel_var = utils.my_standardize(melspec)
        #> Shape: (num_samples,28)

    # generate frames for given recording
    frames = get_frames(melspec_norm,frame_length, frame_step,fs) 
    
    return frames #> a list: len=num_frames; frame[0].shape:(n_melFilt,320) for 5s frame length.


#-------#-------#------- For Sentence level analysis -------#-------#-------#
def sentenceLevel_audio_feature(fileId,n_melFilt,speech_path):
    # fileId: stimulus file Id
    # returns list of sentences; each sentence is an array of size: num_words (of that sent)
    # not returning mismatched sentence features.
    
    # Load signal
    wavFile = f'{speech_path}/16khz_audio{fileId}.wav'
    signal, sig_fs = sf.read(wavFile)
    
    # Load sentence time boundaries
    text_info = np.load(f'{text_path}/refined_Run{fileId}.npz',allow_pickle=True)
    word_onset_time = text_info['onset_time']
    word_offset_time = text_info['offset_time']
    
    # sentence-wise segmentation of audio file 
    sent_onset = [x[0] for x in word_onset_time]
    sent_offset = [x[-1] for x in word_offset_time]
    sent_len = [len(x) for x in word_onset_time]
    segments = segment_signal(signal,sent_onset,sent_offset,sig_fs) #list of sentences
    
    # Compute melFB features for each sentence
    num_sents = len(segments)
    melspec = [] #list of sentences' feature
    for k in range(num_sents):
        feat = melFB(segments[k],sig_fs,n_melFilt) # (use default frame_length,frame_stride to obtain 64Hz fs)
        # Mean and variance normalization
        feat_norm, mel_mean ,mel_var = utils.my_standardize(feat)
        melspec.append(feat_norm) #> feat_norm.Shape: (num_samples,28)
        
    melspec =  [np.transpose(melspec[k]) for k in range(num_sents)] #change shape of each to [num_melFilt,num_timeSamples]

    return melspec, sent_len, word_onset_time, word_offset_time

# speech representation after passing through pre-trained model 
def sentenceLevel_modelpreTrained_audio_feature(fileId,n_melFilt,speech_path):
    # fileId: stimulus file Id
    # returns list of sentences; each sentence is an array of size: num_words (of that sent)
    
    # Load signal
    wavFile = f'{speech_path}/16khz_audio{fileId}.wav'
    signal, sig_fs = sf.read(wavFile)
    
    # Load sentence time boundaries
    text_info = np.load(f'{text_path}/refined_Run{fileId}.npz',allow_pickle=True)
    word_onset_time = text_info['onset_time']
    word_offset_time = text_info['offset_time']
    
    # sentence-wise segmentation of audio file 
    sent_onset = [x[0] for x in word_onset_time]
    sent_offset = [x[-1] for x in word_offset_time]
    segments = segment_signal(signal,sent_onset,sent_offset,sig_fs) #list of sentences
    
    # Compute melFB features for each sentence
    melspec = [] #list of sentences' feature
    for k in range(len(segments)):
        feat = melFB(segments[k],sig_fs,n_melFilt) # (use default frame_length,frame_stride to obtain 64Hz fs)
        # Mean and variance normalization
        feat_norm, mel_mean ,mel_var = utils.my_standardize(feat)
        melspec.append(feat_norm) #> feat_norm.Shape: (num_samples,28)
        
    melspec =  [np.transpose(melspec[k]) for k in range(len(segments))] #change shape of each to [num_melFilt,num_timeSamples]
    
    # Load pretrained neural network 
    model = torch.load('model_set0_frame5.mdl')
    
    #Forward (only the audio subnetwork part)
    # since each sentence length is variable, need to feed each sentence separately
    avg_y_speech = []
    for k in range(len(melspec)):
        feat = melspec[k]
        speech_feat = torch.from_numpy(feat).to(device,dtype=torch.float)
        speech_feat = torch.unsqueeze(speech_feat,0) #add singleton dimension for batch_size
        with torch.no_grad():
            y_speech = model.speech_nw(speech_feat) # output dimensions: 32 x ns' : sentence wise feature
        
        # Obtain feat_dim(32) x num_words representation
        # convert time to index
        onset_index, offset_index = utils.time_to_index(word_onset_time[k],word_offset_time[k],True) #conv2d layer exist (True)
        # average pool along x-axis (time)
        avg_y_speech.append(utils.avg_pooling(y_speech,onset_index,offset_index))
        
    return avg_y_speech

# word2vec representation 
def sentenceLevel_text_feature(fileId,text_path,w2vModel_file):
    # fileId: stimulus file Id
    # returns list of sentences; each sentence is an array of size: num_words (of that sent)
    
    # Load segmented words of each sentence & time boundaries
    text_info = np.load(f'{text_path}/refined_Run{fileId}.npz',allow_pickle=True)
    word_list = text_info['word_list']
    word_onset_time = text_info['onset_time']
    word_offset_time = text_info['offset_time']
    
    # get word2vec model
    wv_model = KeyedVectors.load_word2vec_format(w2vModel_file, binary=True, limit=1000000) #binary=True for bin format & False for txt format

    # Compute word2vec features for each sentence
    absentWords = []
    sentFeat_list = []
    sent_len = []
    for sent in word_list:
        curSent = []
        for word in sent:
            word = word.lower()
            if word in wv_model.key_to_index: # check if the word present in word2vec vocabulary
                feat = wv_model[word]
            else:
                absentWords.append(word)
                feat = np.zeros(300) # NB: 300 is word2vec(google-news) dimension; 
                                     # assigning all-zero vector for words not present word2vec model.
            curSent.append(feat)
        curSent = np.array(curSent).transpose()   
        sentFeat_list.append(torch.tensor(curSent))
        sent_len.append(curSent.shape[1]) #number of words in each sent
        
    with open(f'{text_path}/absentWords.txt', "a") as myfile:
        myfile.write(f'\n** Stimulus File ID-{fileId}:: ')
        for abw in absentWords:
            myfile.write(f'{abw} \t')
    return sentFeat_list, sent_len, word_onset_time, word_offset_time
#------------ Dataset Preparation ------------#
## Speech Data :: returns full list of frames : for the given stimulus files
def prepare_speech_sent(fileList,n_melFilt,speech_path):
    ''' return feature frames of speech stimuli 
        for the files given in fileList.
        fileList: list of file Ids of stimuli (index starting from 0; eg:for all 20 files=0:19)
        Returns:
        speech_frames: list of frames 
            frames are ndarray (of shape: 32 x num_words) nb: num_words is diff for each sent
        
        Note: If you want separate list of frames for each stimulus file,
            use append() instead of extend() to join list.  So,then returns a list of lists 
            
        # Input: 
    '''
    matched_speech_frames = []
    sent_numWords = []
    onset_time = []
    offset_time = []
    for sp_id in fileList:
        # sents, numWords, word_onset_time, word_offset_time = sentenceLevel_text_feature(sp_id+1, text_path,w2vModel_file) #sents - list of nd array

        sents, numWords, word_onset_time, word_offset_time = sentenceLevel_audio_feature(sp_id+1,n_melFilt,speech_path) #sents - list of nd array
        ''' sp_id+1 : audio file Id start from 1, but fileList start from 0'''
        matched_speech_frames.extend(sents)

        sent_numWords.extend(numWords)
        onset_time.extend(word_onset_time)
        offset_time.extend(word_offset_time)

    return matched_speech_frames, sent_numWords, onset_time, offset_time

def prepare_text_sent(fileList, text_path, w2vModel_file):
    ''' return feature frames of text stimuli 
        for the files given in fileList.
        fileList: list of file Ids of stimuli (index starting from 0; eg:for all 20 files=0:19)
        Returns:
        text_frames: list of frames 
            frames are ndarray (of shape: 32 x num_words) nb: num_words is diff for each sent
        
        Note: If you want separate list of frames for each stimulus file,
            - use append() instead of extend() to join list.  So,then returns a list of lists 
            
        # Input: 
    '''
    matched_text_frames = []
    mismatched_text_frames = []
    sent_numWords = []
    onset_time = []
    offset_time = []
    for sp_id in fileList:
        sents, numWords, word_onset_time, word_offset_time = sentenceLevel_text_feature(sp_id+1, text_path,w2vModel_file) #sents - list of nd array
        ''' sp_id+1 : audio file Id start from 1, but fileList start from 0'''
        matched_text_frames.extend(sents)
        sent_numWords.extend(numWords)
        onset_time.extend(word_onset_time)
        offset_time.extend(word_offset_time)
    
        # Get mismatched frames sequence
        # shuffle words within the sentence to generate mismatched sample
        mis_sents = []
        for s in sents: 
            indx = list(range(s.shape[1])) # Debug: Error is some sentences has only 1 word!!
            random.shuffle(indx) 
            mis = s[:,indx]
            mis_sents.append(mis) 
        mismatched_text_frames.extend(mis_sents)

    return matched_text_frames, mismatched_text_frames, sent_numWords, onset_time, offset_time

# ----------------------- EEG Data ----------------------------------#
## EEG data: returns list of sentences for a single subject (full)
def prepare_eegSent_subjectData(fileList, eeg_path, subId,eeg_fs=64):
    ''' return sentences of eeg response for the given subject.  
        fileList: index starts from 0 '''

    fname = f'{eeg_path}/pre_dataSub{subId}.mat'
    resp_data = scipy.io.loadmat(fname,simplify_cells=True,mat_dtype=True)
    sub_data = resp_data['eeg']['data'] #(20,): sub_data[0]-(:,128)

    eeg_frames = []
    for file_id in fileList:
        resp = sub_data[file_id] 
        # Mean and variance normalization
        resp_norm, r_mean ,r_var = utils.my_standardize(resp)
        resp_norm = resp_norm.transpose() # shape:(eeg_channels,time_samples)

        # Load sentence time boundaries
        text_info = np.load(f'{text_path}/refined_Run{file_id+1}.npz',allow_pickle=True)
        word_onset_time = text_info['onset_time']
        word_offset_time = text_info['offset_time']
        
        # sentence-wise segmentation of audio file 
        sent_onset = [x[0] for x in word_onset_time]
        sent_offset = [x[-1] for x in word_offset_time]
        segments = segment_eeg_signal(resp_norm,sent_onset,sent_offset,eeg_fs) #list of sentences

        eeg_frames.extend(segments) # shape of each segment: [num_channels,time_samples]

    return eeg_frames

## EEG data: returns list of frames for a single subject (full)
def prepare_eeg_subjectData(fileList, eeg_path, subId, frame_length,frame_step,fs=64):
    ''' return frames of eeg response for the given subject.  
        fileList: index starts from 0 '''

    fname = f'{eeg_path}/pre_dataSub{subId}.mat'
    resp_data = scipy.io.loadmat(fname,simplify_cells=True,mat_dtype=True)
    sub_data = resp_data['eeg']['data'] #(20,): sub_data[0]-(:,128)

    eeg_frames = []
    for file_id in fileList:
        resp = sub_data[file_id] 
        # Mean and variance normalization
        resp_norm, r_mean ,r_var = utils.my_standardize(resp)
        frames = get_frames(resp_norm,frame_length, frame_step, fs)
        eeg_frames.extend(frames)
    
    return eeg_frames



#---------------- Dataset --------------------------------#
class ourDataset_eegText(Dataset):
    def __init__(self,fileList,subList, eeg_path, text_path, w2vModel_file):
                # Speech data
        # speech_frames, self.onset_time, self.offset_time = prepare_speech_sent(fileList,n_melFilt,speech_path)
        
        # Text Data       
        matched_text_frames, mismatched_text_frames, sent_numWords, self.onset_time, self.offset_time = prepare_text_sent(fileList, text_path, w2vModel_file)
        num_frames = len(matched_text_frames) # number of frames

                        # EEG Data
        # Put together data of all subject 
        # "all_eeg_frames": a list (len=no. of subjects) of list of frames
        all_eeg_frames = []        
        
        for subId in subList: # subject Id starts from 1
            eeg_frames = prepare_eegSent_subjectData(fileList, eeg_path, subId,fs)
            # nb: eeg_frames is a list of frames. Each element in the list is a matrix of size num_eeg channgels x num_time_samples, eg: 128 x 320 for 5s at fs=64Hz.
            all_eeg_frames.append(eeg_frames)
        
        self.resp = all_eeg_frames
        self.stim_match = matched_text_frames
        self.stim_mismatch = mismatched_text_frames
        self.sent_numWords = sent_numWords
        self.num_frames = num_frames
        self.len_matched_resp = len(subList)*num_frames
     
    def __len__(self):
        return self.len_matched_resp*2 #2=> matched and mismatched 
    
    def __getitem__(self,i):
        # Return eeg resp & its stimulus sequence from dataset 
        #   along with match(=1) or mismatch(=0) label
        is_match = math.floor(i/self.len_matched_resp)
        if is_match == 1: 
            i = i % self.len_matched_resp # get remainder: 0 or 1: based on whether i is in [0,len_matched_resp] or [len_matched_resp,2*len_matched_resp]
        sub = math.floor(i/self.num_frames) # get subject id: dividing by num of frames 
        j = i - sub * self.num_frames # frame number
        
        ont = torch.tensor((self.onset_time[j]))
        oft = torch.tensor((self.offset_time[j]))

        if is_match == 1: #matched case
            # tup = (torch.from_numpy(self.resp[i]).float(), torch.from_numpy(self.stim_match[i]).float(), 1)  #1 is label
            tup = (torch.from_numpy(self.resp[sub][j]).float(), self.stim_match[j].float(), 1, self.sent_numWords[j],ont,oft)  #1 is label
        else: #mismatched case
            tup = (torch.from_numpy(self.resp[sub][j]).float(), self.stim_mismatch[j].float(), 0, self.sent_numWords[j],ont,oft)  #0 is label

        return tup

class ourDataset_eegAudio_sent(Dataset):
    # change in mismatch frame selection
    #  choosing immediate next sentence as the mismatch sample for a given sentence
    # Also, change in data variables passed: nw,onset_time and offset_time are passed as columns of an array. (<>_info)
    def __init__(self,fileList,subList, eeg_path, speech_path,n_melFilt):
        # Speech data
        matched_speech_frames, sent_numWords, self.onset_time, self.offset_time  = prepare_speech_sent(fileList,n_melFilt,speech_path)
        num_frames = len(matched_speech_frames) # number of frames
        
        # EEG Data
        # Put together data of all subject 
        # "all_eeg_frames": a list (len=no. of subjects) of list of frames
        all_eeg_frames = []        
        
        for subId in subList: # subject Id starts from 1
            eeg_frames = prepare_eegSent_subjectData(fileList, eeg_path, subId,fs)
            # nb: eeg_frames is a list of frames. Each element in the list is a matrix of size num_eeg channgels x num_time_samples, eg: 128 x 320 for 5s at fs=64Hz.
            all_eeg_frames.append(eeg_frames)
        
        self.resp = all_eeg_frames
        self.stim = matched_speech_frames
        self.sent_numWords = sent_numWords
        self.num_frames = num_frames
        self.len_matched_resp = len(subList)*num_frames
     
    def __len__(self):
        return self.len_matched_resp*2 #2=> matched and mismatched 
    
    def __getitem__(self,i):
        # Return eeg resp & its stimulus sequence from dataset 
        #   along with match(=1) or mismatch(=0) label
        is_match = math.floor(i/self.len_matched_resp) # get  0 or 1: 
        
        i = i % self.len_matched_resp # get remainder:based on whether i is in [0,len_matched_resp] or [len_matched_resp,2*len_matched_resp]
        sub = math.floor(i/self.num_frames) # get subject id: dividing by num of frames 
        frame_id = i - sub * self.num_frames # frame number or i%self.num_frames
        
        # response data is same for match and mismatch cases
        resp_feat = torch.from_numpy(self.resp[sub][frame_id]).float()
        resp_nw = self.sent_numWords[frame_id]
        resp_ont = torch.tensor((self.onset_time[frame_id]))
        resp_oft = torch.tensor((self.offset_time[frame_id]))
        resp_time  = torch.stack((resp_ont,resp_oft),dim=1) # shape: num_words x 2 

        if is_match == 1: #matched case
            # stim parameters
            stim_feat = torch.from_numpy(self.stim[frame_id]).float()
            stim_nw = self.sent_numWords[frame_id]
            stim_ont = torch.tensor((self.onset_time[frame_id]))
            stim_oft = torch.tensor((self.offset_time[frame_id]))
            stim_time  = torch.stack((stim_ont,stim_oft),dim=1) # shape: num_words x 2
            tup = (resp_feat, resp_nw, resp_time, stim_feat, stim_nw, stim_time, 1)  #1 is label
            
        else: #mismatched case
            frame_id_list  = list(range(0,self.num_frames))
            frame_id_list.remove(frame_id) #ensuring matched frame is not selected
            mismatch_frame_id = random.choice(frame_id_list)
            # stim parameters
            stim_feat = torch.from_numpy(self.stim[mismatch_frame_id]).float() # mismatched stim frame for above response
            stim_nw = self.sent_numWords[mismatch_frame_id]
            stim_ont = torch.tensor((self.onset_time[mismatch_frame_id]))
            stim_oft = torch.tensor((self.offset_time[mismatch_frame_id]))
            stim_time  = torch.stack((stim_ont,stim_oft),dim=1) # shape: num_words x 2
            tup = (resp_feat, resp_nw, resp_time, stim_feat, stim_nw, stim_time, 0)  #0 is label
            
        return tup

class ourDataset_audioText(Dataset):
    def __init__(self,fileList, speech_path,text_path, w2vModel_file, n_melFilt):
                # Speech data
        matched_speech_frames, mismatched_speech_frames = prepare_speech_sent(fileList,n_melFilt,speech_path)
        len_matched_resp = len(matched_speech_frames) # number of frames
        
        # Text Data        
        text_frames, sent_numWords = prepare_text_sent(fileList, text_path, w2vModel_file)
        
        self.resp = text_frames
        self.stim_match = matched_speech_frames
        self.stim_mismatch = mismatched_speech_frames
        self.sent_numWords = sent_numWords
        self.len_matched_resp = len_matched_resp
     
    def __len__(self):
        return self.len_matched_resp*2 #2=> matched and mismatched 
    
    def __getitem__(self,i):
        # Return eeg resp & its stimulus sequence from dataset 
        #   along with match(=1) or mismatch(=0) label
        is_match = math.floor(i/self.len_matched_resp)
        if is_match == 1: 
            i = i % self.len_matched_resp # get remainder
            
        if is_match == 1: #matched case
            # tup = (torch.from_numpy(self.resp[i]).float(), torch.from_numpy(self.stim_match[i]).float(), 1)  #1 is label
            tup = (self.resp[i].float(), self.stim_match[i].float(), 1, self.sent_numWords[i])  #1 is label
        else: #mismatched case
            tup = (self.resp[i].float(), self.stim_mismatch[i].float(), 0, self.sent_numWords[i])  #0 is label

        return tup

def my_collate(batch):
    # updated for new ourDataset_eegAudio_sent() - with mismatch selection change
    # pad with zeros : resp data
    resp = [torch.t(item[0]) for item in batch] # t(): transpose 2D tensor: for pad_sequences to work
    resp = pad_sequence(resp, batch_first=True)
    resp = resp.permute(0,2,1) # shape to [batch_size,num_eeg_channels,time_dim]

    stim = [torch.t(item[3]) for item in batch]  # padding speech data now
    stim = pad_sequence(stim, batch_first=True)
    stim = stim.permute(0,2,1) # shape to [batch_size,nmelFilt,time_dim]

    resp_numWords = [item[1] for item in batch]
    resp_numWords = torch.tensor(resp_numWords)
    stim_numWords = [item[4] for item in batch]
    stim_numWords = torch.tensor(stim_numWords)

    resp_time = [item[2] for item in batch] 
    resp_time = pad_sequence(resp_time, batch_first=True) # shape: Batch_size x maxNw x 2
    stim_time = [item[5] for item in batch] 
    stim_time = pad_sequence(stim_time, batch_first=True) # shape: Batch_size x maxNw x 2
        
    targets = [item[6] for item in batch]
    targets = torch.tensor(targets)

    return [resp, resp_numWords, resp_time, stim, stim_numWords, stim_time, targets]


